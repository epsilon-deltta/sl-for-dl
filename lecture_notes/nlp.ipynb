{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "generous-knight",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decreased-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding ,LSTM , Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "ë‹¨ì–´ë¥¼ vectorization í•˜ê¸° í•´ì•¼í•˜ëŠ”ë°.. ğŸ˜’\n",
    "one hot encodding vs word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-appeal",
   "metadata": {},
   "source": [
    "## Word embedding  \n",
    "### Based on Frequency \n",
    "- CountVectorizer : íšŸìˆ˜\n",
    "- TfidfVectorizer : TF-IDF (Term Frequency X Inverse Document Frequency)\n",
    "\n",
    "what i dont know :(LSA??),NPLM ?\n",
    "### Based on Prediction \n",
    "- NPLM  : Bengio et al., A Neural Probabilistic Language Model, 2003\n",
    "- Word2Vec (google): Distributed hypothesis : Tomas Mikolov. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.\n",
    "    - CBow \n",
    "    - Skip-gram  \n",
    "- FastText (facebook)\n",
    "- Glove ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-production",
   "metadata": {},
   "source": [
    "### ABOUT word2vec : Birds of a feather flock together\n",
    "> __ë¹„ìŠ·í•œ ìœ„ì¹˜ì—ì„œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì€ ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤__   \n",
    "distributed representation < distributional hypothesis\n",
    "- Cbow (Continuous Bag of Words)\n",
    "- Skip-gram (has better perpormance compared to CBOW)\n",
    "[problem](https://inspiringpeople.github.io/data%20analysis/word_embedding/)\n",
    "\n",
    "- [ì¥ë‹¨ì ](https://inspiringpeople.github.io/data%20analysis/word_embedding/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-disclosure",
   "metadata": {},
   "source": [
    "##### keras Embedding Layer   \n",
    "Embedding(words set size,embedding vector size,input_dim=)  \n",
    "[ 13,3,5,7 ] - > [ [12,...,12] ,[23,...,44], [12,...,12] ,[23,...,44] ]  \n",
    "Q. [vector,vector,..] ë„ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°ˆìˆ˜ìˆëŠ”ê°€ ? (pretrained ì‚¬ìš©í•˜ê³  ë˜ ì„ë² ë”© ì¡°ì§ˆë ¤ê³ )  \n",
    "vector ëŠ” ì•ˆë˜ê³  ì •ìˆ˜ ì¸ì½”ë”©ë§Œ ëœë‹¨ë‹¤ : [https://wikidocs.net/33793](https://wikidocs.net/33793)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-nevada",
   "metadata": {},
   "source": [
    "### Pre-trained Embedding Model\n",
    "korean\n",
    "- word2vec :\n",
    "    - [ë°•ê·œë³‘ë‹˜(2017ë…„ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸,(30185ê°œ, 200ì°¨ì›))](https://github.com/Kyubyong/wordvectors)\n",
    "- FastText :\n",
    "    - [model](https://fasttext.cc/docs/en/crawl-vectors.html) , [.whl (pip install file.whl)(fasttext,it doesn't work well in windows)](https://www.lfd.uci.edu/~gohlke/pythonlibs/#fasttext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "one hot encoding : ë ë°œ ë©”ëª¨ë¦¬ ë„˜ì¹ ì¼ìˆëƒ \n",
    "word embedding : ê·¸ë˜ ë¨¼ê°€ ì¢€ ë¹„ìŠ·í•œ ë†ˆë¼ë¦¬ ë²¡í„° ìœ ì‚¬ë„ë„ ê°–ê³  í•´ì•¼ì§€ ëª¨ë¸ì´ ì´í•´ë¥¼ í• ê±° ì•„ì´ê°€ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-package",
   "metadata": {},
   "source": [
    "### How to make Word Embedding ..\n",
    "ë ë¶€ë„ ì¢‹ì•„ë³´ì´ê¸´í•˜ëŠ”ë° ì–´ë–»ê²Œ ì„ë² ë”© ë•Œë¦¬ëƒ... ~~ì›í•«ì¸ì½”ê°€ ì‰½ê¸´í•œë°~~ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-uniform",
   "metadata": {},
   "source": [
    "### ways of word Embedding\n",
    "LSA, Word2Vec, FastText, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-thermal",
   "metadata": {},
   "source": [
    "Time to Hunt Word2Vec , Doc2Vec  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-rough",
   "metadata": {},
   "source": [
    "### Sequantial Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-bikini",
   "metadata": {},
   "source": [
    "- lstm : [S.Hochreiterì™€ J.Schmidhuberê°€ 1997ë…„ì— ì œì•ˆí•œ ì…€](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735#.WIxuWvErJnw)\n",
    "- GRU : [Learning Phrase Representations using RNN Encoderâ€“Decoder\n",
    "for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)                 - Seq2Seq    \n",
    "- Seq2Seq with Attention   \n",
    "- Transformer [Attention is All we need](https://arxiv.org/abs/1706.03762)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-familiar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
